# Decomposition of Matrix

Artificial Intelligence has holistic origins - the result of our long optimized evolution, which works a lot like a Deep Learning engine with evolving optimizer; while there is random chance in our genes: this is just a philosophy, a solid and stable systems distribute even chaos with random factors, and actually cover their combinatoric space evenly. The "chance", even a fatal mutation, if like a random factor in light: we can still estimate light very presicely and the inherent random element is perfectly part of the equation. These random elements optimize the space of themselves, becoming more or less common through natural choices: we can say it's a part of larger mechanism not meant to be random, but to cancel this random element out by it's distribution and statistical means, as well as combinatoric distribution. This randomness, no way removes determination: this is Deep Learning, the functioning of our brains to conquer this reaction into balanced optimizers of Principles of Life, the Acceleration from this chaotic evolution of matter and mind.

Matrix, when it has input of the same vector twice, or the same vector types, shows how we take each combination of the vector elements or axes of a tensor. We combine each with each.

For a three vector of elements X, Y and Z, which reminds of 3D space, from initial vector we can use matrix to get the final vector.

Each combination is a multiplication: if both elements are ones, it gives one, and if one or both are zeroes, it gives zero; thus, in such binary space the common matrix reminds of the AND operation; so we can say it's a fuzzy combinator for AND.

It contains addition - three times, three components are summed, where their multiplication is included. As in identity matrix, one and two zeroes highlights one component of the combination; while three times 1/3 distributes them evenly. This means: in very straightforward way, we can assimilate this addition on one axe and multiplication on two axes; notice from my math: multiplication is nothing but a two-dimensional addition. As the one-dimensional space correlates to addition, and two-dimensional space to multiplication, the combinatorics works out perfectly: the metaspace for each operation is the same; so we kind of do *only* the multiplication.

Matrix with input and output vector for it's multiplication, then, gives all combinations between two vectors. For example, you have eggs and chicken and you combine each egg with each chicken.

We see the matrices form synapses: potential for each synaps between each two nerves turns them to digital nerve networks.

Combinations form a symmetric space: decomposition of matrix into a fuzzy combinator of possible combinations gives us powers:
- We can resize, blur, ensharpen matrix and vector lengths and side lengths appropriately; the smaller-density matrix is an approximation, which might get lost by vibrating qualities of larger matrix, but it's very uniformly distributed.
- We can turn it into a logic.
- We can find it's basic mathematical elements: this means, we want to analyze other combinators than matrices.

It compares to machine learning: this, in turn, optimizes other general, symmetric formulaes, such as linear equation for linear regression or logarithmic equation for logarithmic regression.

We see the optimizer: two-dimensional space of dimensions in matrix are linearized through componentized differentiation, and despite this high space, it seems to approach solutions, especially simpler ones, quite linearly.

# Activation

Where to run parallels with activation?

In my math, I use equation of dimensional and value space, R and T of a number:
- 4 * 4 matrix, for example forms 4 * 4 space of a number.
- 16 values in 4 * 4 value space would yield into fitting space of the values.

If the same matrix is applied to space of a number, and values of a number, which behave differently, we see how the result and the source would go through the same calculation. 4 * 4 metaspace could be added for third frequency, where it's moving inside imaginary space.

Such fractal space would be mathematically analogous to this: activation function, through loss of certain information makes a generalization.

For example, imagine 25\*25 dimensional space, where you have 5\*5 matrix with fractal multiplication into each cell being 5*5 cell.

Imagine, you now average and remove the inner 5*5 cell: this could be your value space.

Now, the values could be generalized: tough they are over large area, the simpler area becomes very rigid. You could move the selected inner component to another part of number, for example outer 5\*5 in your information cell, while inner position of 5\*5 matrix in an inner cell - this could remove any information loss by giving you power over weights of these.

When matrix is rotating through this generalized space, the result might not be linear: it can curve and modify the space until the complexity grows quite large.

From input (the digit space) to output (the value space): if digit space (digits in a number) and value space (n of the base-n number) are of equal precision, the two weight and bias systems can comprehend input and output, and thus this should be a non-linear system; but at the same time it could be simpler, than activation functions: complex number could bend matrix asymmetries between number space and value space.

What this principle conveys?

Non-linearity has to do about acceleration and decceleration: in non-linear spaces, each point exists in accelerating space and thus, the numbers are bigger or smaller than fits into one window of frequencies; acceleration and decceleration produce point values different from linear lines. You won't notice this in original Gaussian solutions, while you can use his math and logic very well - especially the principles, which still hold (as a proven math, despite from different system). Point acceleration means that input and output are not linearly distributed, but from perspective of output, the space towards input can appear longer or shorter, or somehow inversed or reversed, it's curved in terms of it's length as measured before and after.

In my example this curve occurs in static point, but it can be smooth and produce a fractal function of input and output.

This idea is very basic and for both of these ideas, you see many ways to implement: this is not a particular architecture, but a general principle. You might do this with completely different equation - to decompose the activation principle and understand the precise math essence, how to get to non-linearity in each case.

# Combinations

To do much more efficient matrices in infinity:
- You have same number of coordinates towards outer space, as towards inner space. In my math, the principles are covered as well as one ideal number type for this, based on it's simplicity.

# Multifrequencies

Accelerations and deccelerations: this is the big linearity where your simple A.I. can fall; it must learn too much to overcome this.

You need to learn the relations, which happen in this type of a number space:
- Imagine taking levels of integration and differentiation; you create nivels less than one level, such as finding a digit, which is 1/10 of one octave in one integration or differentiation. This number space is one dimension of your number - normal order of digits gives the other one. You normalize both so that they are in common number space: in infinity, they remain rhytmic. Refer to my math to achieve all this on one framework of digits, since you need many systems to play with, you need some common standards.

Where you measure three rows in your dataset, in addition to formulae of the numbers, you find acceleration or decceleration factors, if not a linear factor. In my mathematical space, so many number effects are covered that these might lead to arbitrary complexity, arbitrarly shaped curves: with less digits and more simplicity, you can create combinatorial space to solve these things on more general or specific level, which involves numbers of different digit lengths, as well as your basic relation, which are the relations under your focus you preserve in your numbers.

# Decomposition of an A.I.

We try to:
- Deconstruct the AI, as much as it's based on holistic evolution of brain, not containing single factors, but a coherent whole.
- Create a syntax or set of decomposed factors, and essential principles, which allow us to talk about an A.I. in simple, logical terms.
